# DRL_Final
Final project for Deep Reinforcement Learning 2025@NTU


# HOW TO USE RL ENVIRONMENT

## ğŸ“ Project Structure

    â”œâ”€â”€ abc/                         # Cloned ABC logic synthesis tool
    â”œâ”€â”€ abc.history                  # ABC command history (optional)
    â”œâ”€â”€ helper_functions.py          # Internal utility functions (âš ï¸ Do not use directly)
    â”œâ”€â”€ requirements.txt             # Python dependencies
    â”œâ”€â”€ rl_environment.py            # Main environment entry point (âœ… use this)
    â”œâ”€â”€ run_default_command.py       # Run built-in command in ABC(including resyn2 and deepsyn simulation)
    â”œâ”€â”€ runtime_results/             # Output directory
    â”‚   â””â”€â”€ current.aig              # Example AIG file
    â””â”€â”€ testbenches/                 # Benchmark circuits
        â”œâ”€â”€ 2025_IWLS_Contest_Benchmarks_020425/
        â””â”€â”€ EPFL/


### 1. Clone the ABC Library

You must clone and build the ABC logic synthesis tool in the `abc/` folder:

    git clone https://github.com/berkeley-abc/abc.git abc
    cd abc
    make
    cd ..

### 2. Install Python Dependencies

Use the provided `requirements.txt` to install the required packages:

    pip install -r requirements.txt

### 3. Run the Environment

Use `rl_environment.py` as the main entry point. It includes a simple demo in the `main()` function:

    python rl_environment.py

## âš ï¸ Notes

- Do not modify or run `helper_functions.py` directly. This file contains internal utilities and currently has unresolved issues.
- Only use the `rl_environment.py` interface for interaction and extension.




